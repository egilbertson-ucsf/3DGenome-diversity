{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pysam\n",
    "\n",
    "from basenji import dataset, dna_io, seqnn\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some core functions that are used for prediction from sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAkita():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "    import tensorflow as tf\n",
    "    if tf.__version__[0] == '1':\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "    with open('/wynton/group/capra/projects/pan_3d_genome/data/model/params.json') as params_file:\n",
    "        params = json.load(params_file)\n",
    "        params_model = params['model']\n",
    "        params_train = params['train']\n",
    "        \n",
    "    global seqnn_model\n",
    "    seqnn_model = seqnn.SeqNN(params_model)\n",
    "    seqnn_model.restore('/wynton/group/capra/projects/pan_3d_genome/data/model/model_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           [(None, 1048576, 4)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stochastic_reverse_complement ( ((None, 1048576, 4), 0           sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stochastic_shift (StochasticShi (None, 1048576, 4)   0           stochastic_reverse_complement[0][\n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 1048576, 4)   0           stochastic_shift[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1048576, 96)  4224        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1048576, 96)  384         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 524288, 96)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 524288, 96)   0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 524288, 96)   46080       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 524288, 96)   384         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 262144, 96)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 262144, 96)   0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 262144, 96)   46080       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 262144, 96)   384         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 131072, 96)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 131072, 96)   0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 131072, 96)   46080       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 131072, 96)   384         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 65536, 96)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 65536, 96)    0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 65536, 96)    46080       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 65536, 96)    384         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 32768, 96)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32768, 96)    0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 32768, 96)    46080       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32768, 96)    384         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 16384, 96)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 16384, 96)    0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16384, 96)    46080       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16384, 96)    384         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 8192, 96)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 8192, 96)     0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 8192, 96)     46080       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8192, 96)     384         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 4096, 96)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 4096, 96)     0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 4096, 96)     46080       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4096, 96)     384         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 2048, 96)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 2048, 96)     0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2048, 96)     46080       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2048, 96)     384         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1024, 96)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 1024, 96)     0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1024, 96)     46080       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1024, 96)     384         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 512, 96)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 512, 96)      0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 48)      13824       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 512, 48)      192         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 512, 48)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 96)      4608        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512, 96)      384         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 96)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 512, 96)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 512, 96)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 512, 48)      13824       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 512, 48)      192         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 512, 48)      0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 512, 96)      4608        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 512, 96)      384         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 96)      0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512, 96)      0           add[0][0]                        \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 512, 96)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 512, 48)      13824       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 512, 48)      192         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 512, 48)      0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 512, 96)      4608        re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 512, 96)      384         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 96)      0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 512, 96)      0           add_1[0][0]                      \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 512, 96)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 512, 48)      13824       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 512, 48)      192         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 512, 48)      0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 512, 96)      4608        re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 512, 96)      384         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 96)      0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 512, 96)      0           add_2[0][0]                      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 512, 96)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 512, 48)      13824       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 512, 48)      192         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 512, 48)      0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 512, 96)      4608        re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 512, 96)      384         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512, 96)      0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 512, 96)      0           add_3[0][0]                      \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 512, 96)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 512, 48)      13824       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 512, 48)      192         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 512, 48)      0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 512, 96)      4608        re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512, 96)      384         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512, 96)      0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 512, 96)      0           add_4[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 512, 96)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 512, 48)      13824       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 512, 48)      192         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 512, 48)      0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 512, 96)      4608        re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 512, 96)      384         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512, 96)      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 512, 96)      0           add_5[0][0]                      \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 512, 96)      0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 512, 48)      13824       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 512, 48)      192         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 512, 48)      0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 512, 96)      4608        re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 512, 96)      384         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512, 96)      0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 96)      0           add_6[0][0]                      \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 512, 96)      0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 512, 64)      30720       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 512, 64)      256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 512, 64)      0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "one_to_two (OneToTwo)           (None, 512, 512, 64) 0           re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_dist2d (ConcatDist2D)    (None, 512, 512, 65) 0           one_to_two[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 512, 512, 65) 0           concat_dist2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 48) 28080       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 512, 512, 48) 192         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d (Symmetrize2D)     (None, 512, 512, 48) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 512, 512, 24) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 48) 1152        re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 512, 512, 48) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 512, 512, 48) 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 512, 512, 48) 0           symmetrize2d[0][0]               \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_1 (Symmetrize2D)   (None, 512, 512, 48) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 512, 512, 24) 96          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 512, 512, 48) 1152        re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 512, 512, 48) 192         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512, 512, 48) 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 512, 512, 48) 0           symmetrize2d_1[0][0]             \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_2 (Symmetrize2D)   (None, 512, 512, 48) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 512, 512, 24) 96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 512, 512, 48) 1152        re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 512, 512, 48) 192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512, 512, 48) 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_2[0][0]             \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_3 (Symmetrize2D)   (None, 512, 512, 48) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 512, 512, 24) 96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 512, 512, 48) 1152        re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 512, 512, 48) 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512, 512, 48) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_3[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_4 (Symmetrize2D)   (None, 512, 512, 48) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 512, 512, 24) 96          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 512, 512, 48) 1152        re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 512, 512, 48) 192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 512, 512, 48) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_4[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_5 (Symmetrize2D)   (None, 512, 512, 48) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 512, 512, 24) 10368       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 512, 512, 24) 96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 512, 512, 48) 1152        re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512, 512, 48) 192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 512, 512, 48) 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_5[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_6 (Symmetrize2D)   (None, 512, 512, 48) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 448, 448, 48) 0           symmetrize2d_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "upper_tri (UpperTri)            (None, 99681, 48)    0           cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 99681, 5)     245         upper_tri[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "switch_reverse_triu (SwitchReve (None, 99681, 5)     0           dense[0][0]                      \n",
      "                                                                 stochastic_reverse_complement[0][\n",
      "==================================================================================================\n",
      "Total params: 751,653\n",
      "Trainable params: 746,149\n",
      "Non-trainable params: 5,504\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [2048]\n",
      "target_lengths [99681]\n",
      "target_crops [-49585]\n"
     ]
    }
   ],
   "source": [
    "loadAkita()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAkitaPreds(seq):\n",
    "    if len(seq) != 2**20: raise ValueError('len(seq) != seq_length')\n",
    "    seq_1hot = dna_io.dna_1hot(seq)\n",
    "    test_pred_from_seq = seqnn_model.model.predict(np.expand_dims(seq_1hot,0))\n",
    "    return test_pred_from_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparePreds(ref, alt):\n",
    "    mse = np.mean(np.square(ref - alt))\n",
    "    spearman = stats.spearmanr(ref, alt)[0]\n",
    "    divergence = 1 - spearman\n",
    "    return (mse, divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for in silico mutagenesis SNV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contact_maps():\n",
    "    with open(f'/wynton/group/capra/projects/pan_3d_genome/data/predictions/reference/3d_predictions_HFF_{chr}.txt', 'r') as ref_file:\n",
    "        for line in ref_file:\n",
    "            ref_line = line.split('\\t')\n",
    "            if ref_line[0] == chr and ref_line[1] == window_start:\n",
    "                print(ref_line[0])\n",
    "                print(ref_line[1])\n",
    "                ref_pred_hff = list(map(float,ref_line[2:]))\n",
    "\n",
    "    ref_fasta_open = pysam.Fastafile('/wynton/group/capra/projects/pan_3d_genome/data/reference_fasta/filtered_panTro6.fa')\n",
    "    ref_seq = ref_fasta_open.fetch(chr, int(window_start), int(window_start)+2**20).upper()\n",
    "    pos_in_window = (int(variant_pos)-1)-int(window_start)\n",
    "    alt_seq = ref_seq[0:pos_in_window] + alt_allele + ref_seq[pos_in_window+1:]\n",
    "    alt_pred = runAkitaPreds(alt_seq)\n",
    "    alt_pred_hff = alt_pred[:,:,0][0]\n",
    "    return ref_pred_hff, alt_pred_hff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_silico_mutagenesis_SNV_test(chromosome,pos,alt,target_window):\n",
    "    global chr\n",
    "    chr = chromosome\n",
    "    \n",
    "    global variant_pos\n",
    "    variant_pos = pos\n",
    "    \n",
    "    global alt_allele\n",
    "    alt_allele = alt\n",
    "    \n",
    "    global window\n",
    "    window = target_window\n",
    "    \n",
    "    global window_start\n",
    "    window_start = window.split('_')[1]\n",
    "    \n",
    "    ref_prediction, alt_prediction = get_contact_maps()\n",
    "    mse, divergence = comparePreds(np.array(ref_prediction), np.array(alt_prediction))\n",
    "    return mse, divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr13\n",
      "44564480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0004109143222303805, 0.014099135528463869)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_silico_mutagenesis_SNV_test('chr13',44760355,'C','chr13_44564480')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr13\n",
      "45088768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.6360405091583755e-11, 1.1632317331589093e-09)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_silico_mutagenesis_SNV_test('chr13',45090113,'T','chr13_45088768')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr13\n",
      "3145728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0089372026745314e-10, 3.3496874163319035e-09)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_silico_mutagenesis_SNV_test('chr13',3145764,'T','chr13_3145728')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for in silico mutagenesis inversions script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_map():\n",
    "    ref_fasta_open = pysam.Fastafile('/wynton/group/capra/projects/pan_3d_genome/data/reference_fasta/filtered_panTro6.fa')\n",
    "    ref_seq = ref_fasta_open.fetch(chr, int(window_start), int(window_start)+2**20).upper()\n",
    "\n",
    "    inv_start_in_window = (int(inv_start)-int(window_start))\n",
    "    inv_end_in_window = (int(inv_end)-int(window_start))\n",
    "    seq_to_invert = ref_seq[inv_start_in_window:inv_end_in_window]\n",
    "    inverted_seq = seq_to_invert[::-1]\n",
    "    alt_seq = ref_seq[0:inv_start_in_window] + inverted_seq + ref_seq[inv_end_in_window:]\n",
    "    alt_pred = runAkitaPreds(alt_seq)\n",
    "    alt_pred = alt_pred[:,:,0][0]\n",
    "    return alt_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref_map():\n",
    "    with open(f'/wynton/group/capra/projects/pan_3d_genome/data/predictions/reference/3d_predictions_HFF_{chr}.txt', 'r') as ref_file:\n",
    "        for line in ref_file:\n",
    "            if window_start in line:\n",
    "                ref_line = line.split('\\t')\n",
    "                ref_pred = list(map(float,ref_line[2:]))\n",
    "    return ref_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185597952\n",
      "Akita prediction for window 185597952 with inversion chr1: 186157349-186164563 completed.\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "\n",
    "with open(f'../../data/Porubsky_et_al_2020/example.bed', 'r') as input, open(f'results.txt', 'w') as out:\n",
    "    for line in input:\n",
    "        line = line.split('\\t')\n",
    "            \n",
    "        global chr\n",
    "        chr = line[0]\n",
    "\n",
    "        global inv_start\n",
    "        inv_start = line[1]\n",
    "\n",
    "        global inv_end\n",
    "        inv_end = line[2]\n",
    "\n",
    "        global window_start\n",
    "        window_start = line[3].strip()\n",
    "\n",
    "        alt_pred = get_alt_map()\n",
    "        print(window_start)\n",
    "        ref_pred = load_ref_map()\n",
    "        mse, pearson, spearman = compare3Dmaps(ref_pred, alt_pred)\n",
    "\n",
    "        df.append({'chr':chr,'start':int(inv_start),'end':int(inv_end),'window':int(window_start),'mse':mse,'1-pearson':pearson,'1-spearman':spearman})\n",
    "        print(f'Akita prediction for window {window_start} with inversion {chr}: {inv_start}-{inv_end} completed.')\n",
    "\n",
    "        if spearman >= 0.01:\n",
    "            with open(f'/wynton/group/capra/projects/pan_3d_genome/data/Porubsky_et_al_2020/inversion_predictions/{chr}_{inv_start}_{window_start}.txt', 'w') as pred_out:\n",
    "                pred_out.write(chr + \"\\t\" + str(inv_start) + \"\\t\" + \"\\t\".join([str(x) for x in alt_pred]))\n",
    "\n",
    "    df = pd.DataFrame(df)[['chr','start','end','window','mse','1-pearson','1-spearman']]\n",
    "    df.to_csv(out, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02302839970757009"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.042321533,\n",
       " 0.04950109,\n",
       " 0.049550265,\n",
       " 0.051678896,\n",
       " 0.05693528,\n",
       " 0.06291446,\n",
       " 0.06928441,\n",
       " 0.07436684,\n",
       " 0.079781264,\n",
       " 0.08680591,\n",
       " 0.09564158,\n",
       " 0.10340151,\n",
       " 0.10618511,\n",
       " 0.10733941,\n",
       " 0.11223081,\n",
       " 0.112941414,\n",
       " 0.11043611,\n",
       " 0.1095058,\n",
       " 0.10626182,\n",
       " 0.103426665,\n",
       " 0.0943093,\n",
       " 0.08957437,\n",
       " 0.0859817,\n",
       " 0.08232996,\n",
       " 0.07758829,\n",
       " 0.07581434,\n",
       " 0.080748945,\n",
       " 0.10042378,\n",
       " 0.118136376,\n",
       " 0.13834044,\n",
       " 0.15040538,\n",
       " 0.15905139,\n",
       " 0.16446969,\n",
       " 0.16588488,\n",
       " 0.17167863,\n",
       " 0.1741775,\n",
       " 0.17216137,\n",
       " 0.16942212,\n",
       " 0.1533142,\n",
       " 0.13449094,\n",
       " 0.111514956,\n",
       " 0.0961034,\n",
       " 0.08350113,\n",
       " 0.07733294,\n",
       " 0.07259634,\n",
       " 0.042252213,\n",
       " -0.009914488,\n",
       " -0.07685408,\n",
       " -0.12591603,\n",
       " -0.16656533,\n",
       " -0.19786733,\n",
       " -0.21263045,\n",
       " -0.22486229,\n",
       " -0.23924942,\n",
       " -0.25371313,\n",
       " -0.2661047,\n",
       " -0.2680828,\n",
       " -0.2724446,\n",
       " -0.27890155,\n",
       " -0.28602973,\n",
       " -0.28879505,\n",
       " -0.2874979,\n",
       " -0.2977839,\n",
       " -0.30759743,\n",
       " -0.31771496,\n",
       " -0.3211323,\n",
       " -0.33061904,\n",
       " -0.3331777,\n",
       " -0.33702007,\n",
       " -0.33685333,\n",
       " -0.33755425,\n",
       " -0.34171572,\n",
       " -0.33781585,\n",
       " -0.34487364,\n",
       " -0.34706503,\n",
       " -0.35257548,\n",
       " -0.34650812,\n",
       " -0.33629203,\n",
       " -0.33156037,\n",
       " -0.325641,\n",
       " -0.31964627,\n",
       " -0.30578855,\n",
       " -0.29828122,\n",
       " -0.29180026,\n",
       " -0.28779972,\n",
       " -0.28293318,\n",
       " -0.2853526,\n",
       " -0.29022598,\n",
       " -0.29064763,\n",
       " -0.28520918,\n",
       " -0.27577883,\n",
       " -0.27194506,\n",
       " -0.26884827,\n",
       " -0.26021862,\n",
       " -0.25434977,\n",
       " -0.25482786,\n",
       " -0.25626355,\n",
       " -0.25561905,\n",
       " -0.2516883,\n",
       " -0.25655484,\n",
       " -0.25863823,\n",
       " -0.26099774,\n",
       " -0.26098764,\n",
       " -0.25996304,\n",
       " -0.25998122,\n",
       " -0.2562558,\n",
       " -0.25412625,\n",
       " -0.25183782,\n",
       " -0.24948458,\n",
       " -0.24693474,\n",
       " -0.24062642,\n",
       " -0.24244681,\n",
       " -0.23813233,\n",
       " -0.23631816,\n",
       " -0.23401482,\n",
       " -0.23269886,\n",
       " -0.2374363,\n",
       " -0.23344383,\n",
       " -0.23023118,\n",
       " -0.22554195,\n",
       " -0.22469398,\n",
       " -0.23051417,\n",
       " -0.229033,\n",
       " -0.22824709,\n",
       " -0.22718298,\n",
       " -0.22995637,\n",
       " -0.22697257,\n",
       " -0.21691024,\n",
       " -0.21218999,\n",
       " -0.20654708,\n",
       " -0.20995936,\n",
       " -0.20766446,\n",
       " -0.2038457,\n",
       " -0.2009179,\n",
       " -0.19594768,\n",
       " -0.1955327,\n",
       " -0.189504,\n",
       " -0.18967658,\n",
       " -0.19084835,\n",
       " -0.19422254,\n",
       " -0.1961698,\n",
       " -0.1934931,\n",
       " -0.1935586,\n",
       " -0.18889919,\n",
       " -0.19061774,\n",
       " -0.18395352,\n",
       " -0.1822001,\n",
       " -0.1760335,\n",
       " -0.17408681,\n",
       " -0.16957319,\n",
       " -0.16312248,\n",
       " -0.16569713,\n",
       " -0.15978998,\n",
       " -0.16019076,\n",
       " -0.15507317,\n",
       " -0.15004534,\n",
       " -0.14880216,\n",
       " -0.146471,\n",
       " -0.14970145,\n",
       " -0.15238872,\n",
       " -0.15448657,\n",
       " -0.1562548,\n",
       " -0.15545672,\n",
       " -0.14550138,\n",
       " -0.14535013,\n",
       " -0.14021432,\n",
       " -0.14235038,\n",
       " -0.14676651,\n",
       " -0.14516184,\n",
       " -0.14775902,\n",
       " -0.1417126,\n",
       " -0.13781041,\n",
       " -0.13547245,\n",
       " -0.12938556,\n",
       " -0.12912458,\n",
       " -0.12711975,\n",
       " -0.12916622,\n",
       " -0.13284603,\n",
       " -0.13029262,\n",
       " -0.12271419,\n",
       " -0.12446338,\n",
       " -0.12264234,\n",
       " -0.127022,\n",
       " -0.11961877,\n",
       " -0.11624846,\n",
       " -0.109057665,\n",
       " -0.10575318,\n",
       " -0.104436964,\n",
       " -0.10570833,\n",
       " -0.10303578,\n",
       " -0.09663707,\n",
       " -0.08878785,\n",
       " -0.08502892,\n",
       " -0.085949,\n",
       " -0.082972854,\n",
       " -0.0770382,\n",
       " -0.067744225,\n",
       " -0.06345713,\n",
       " -0.06173107,\n",
       " -0.05837497,\n",
       " -0.06168738,\n",
       " -0.05922717,\n",
       " -0.060243696,\n",
       " -0.057160348,\n",
       " -0.05280617,\n",
       " -0.050628036,\n",
       " -0.046860576,\n",
       " -0.042265445,\n",
       " -0.037835866,\n",
       " -0.03814456,\n",
       " -0.040812522,\n",
       " -0.03845048,\n",
       " -0.025574267,\n",
       " -0.021414459,\n",
       " -0.017177165,\n",
       " -0.01627019,\n",
       " -0.010849357,\n",
       " -0.004530996,\n",
       " -0.0043064356,\n",
       " -0.00053554773,\n",
       " 0.0024482906,\n",
       " 0.008554608,\n",
       " 0.013038665,\n",
       " 0.017212182,\n",
       " 0.0235475,\n",
       " 0.024150282,\n",
       " 0.019907206,\n",
       " 0.018487722,\n",
       " 0.01752171,\n",
       " 0.017003804,\n",
       " 0.012138963,\n",
       " 0.008692473,\n",
       " 0.011921525,\n",
       " 0.017491817,\n",
       " 0.017156214,\n",
       " 0.014335424,\n",
       " 0.010795683,\n",
       " 0.01023075,\n",
       " 0.012103409,\n",
       " 0.011510074,\n",
       " 0.012163162,\n",
       " 0.011749864,\n",
       " 0.01478079,\n",
       " 0.01542297,\n",
       " 0.01843658,\n",
       " 0.022684783,\n",
       " 0.023488939,\n",
       " 0.024224252,\n",
       " 0.024033397,\n",
       " 0.029015511,\n",
       " 0.030714601,\n",
       " 0.031523436,\n",
       " 0.030893147,\n",
       " 0.035216898,\n",
       " 0.041174322,\n",
       " 0.04524377,\n",
       " 0.049870223,\n",
       " 0.049125046,\n",
       " 0.053750187,\n",
       " 0.054970175,\n",
       " 0.05845675,\n",
       " 0.063479275,\n",
       " 0.068406016,\n",
       " 0.07327345,\n",
       " 0.07441667,\n",
       " 0.08153537,\n",
       " 0.084190756,\n",
       " 0.089689285,\n",
       " 0.09273544,\n",
       " 0.09613356,\n",
       " 0.104557544,\n",
       " 0.1117042,\n",
       " 0.120171815,\n",
       " 0.12338504,\n",
       " 0.13158831,\n",
       " 0.13437662,\n",
       " 0.13425168,\n",
       " 0.13411644,\n",
       " 0.13295081,\n",
       " 0.13764897,\n",
       " 0.13867465,\n",
       " 0.14137617,\n",
       " 0.14164272,\n",
       " 0.14463857,\n",
       " 0.14952615,\n",
       " 0.1504738,\n",
       " 0.15742365,\n",
       " 0.16188928,\n",
       " 0.1721395,\n",
       " 0.17824659,\n",
       " 0.18639931,\n",
       " 0.19144735,\n",
       " 0.19488528,\n",
       " 0.19430867,\n",
       " 0.19088426,\n",
       " 0.19100592,\n",
       " 0.19379547,\n",
       " 0.20047054,\n",
       " 0.20304164,\n",
       " 0.20633629,\n",
       " 0.21446308,\n",
       " 0.23257855,\n",
       " 0.24707279,\n",
       " 0.25792792,\n",
       " 0.26885572,\n",
       " 0.2810975,\n",
       " 0.29020837,\n",
       " 0.29630026,\n",
       " 0.30641052,\n",
       " 0.32206222,\n",
       " 0.33909968,\n",
       " 0.34711763,\n",
       " 0.35376075,\n",
       " 0.3605475,\n",
       " 0.37414274,\n",
       " 0.39127645,\n",
       " 0.40987316,\n",
       " 0.43756267,\n",
       " 0.45333973,\n",
       " 0.45372126,\n",
       " 0.4236426,\n",
       " 0.39384702,\n",
       " 0.36378804,\n",
       " 0.3409222,\n",
       " 0.30980518,\n",
       " 0.28909513,\n",
       " 0.2791935,\n",
       " 0.26323017,\n",
       " 0.24867573,\n",
       " 0.23133174,\n",
       " 0.22141472,\n",
       " 0.2060841,\n",
       " 0.18652639,\n",
       " 0.17169896,\n",
       " 0.15515819,\n",
       " 0.14976439,\n",
       " 0.13722691,\n",
       " 0.12948653,\n",
       " 0.12728015,\n",
       " 0.12584099,\n",
       " 0.12259439,\n",
       " 0.118146986,\n",
       " 0.11263147,\n",
       " 0.10928759,\n",
       " 0.10857776,\n",
       " 0.11365619,\n",
       " 0.1268619,\n",
       " 0.14174303,\n",
       " 0.15817448,\n",
       " 0.169162,\n",
       " 0.17713824,\n",
       " 0.18292996,\n",
       " 0.19080165,\n",
       " 0.20026317,\n",
       " 0.21226284,\n",
       " 0.2222062,\n",
       " 0.22959337,\n",
       " 0.24523142,\n",
       " 0.2493091,\n",
       " 0.25766507,\n",
       " 0.25703672,\n",
       " 0.2554643,\n",
       " 0.24854878,\n",
       " 0.24029121,\n",
       " 0.22927245,\n",
       " 0.21184853,\n",
       " 0.19499204,\n",
       " 0.17790851,\n",
       " 0.16886005,\n",
       " 0.15312931,\n",
       " 0.13141844,\n",
       " 0.10965958,\n",
       " 0.09407613,\n",
       " 0.07897702,\n",
       " 0.063803524,\n",
       " 0.055481285,\n",
       " 0.05208847,\n",
       " 0.047211558,\n",
       " 0.036644965,\n",
       " 0.01411292,\n",
       " -0.006259948,\n",
       " -0.027094513,\n",
       " -0.04715982,\n",
       " -0.071476966,\n",
       " -0.08854982,\n",
       " -0.11311594,\n",
       " -0.12535584,\n",
       " -0.13835806,\n",
       " -0.13739103,\n",
       " -0.14322975,\n",
       " -0.15333813,\n",
       " -0.16462627,\n",
       " -0.16861093,\n",
       " -0.16255379,\n",
       " -0.16822326,\n",
       " -0.1782513,\n",
       " -0.18287763,\n",
       " -0.18280336,\n",
       " -0.18143794,\n",
       " -0.19320986,\n",
       " -0.20278683,\n",
       " -0.20746438,\n",
       " -0.21151976,\n",
       " -0.21858336,\n",
       " -0.22481166,\n",
       " -0.2273108,\n",
       " -0.22465461,\n",
       " -0.21939355,\n",
       " -0.21455204,\n",
       " -0.2078404,\n",
       " -0.1986605,\n",
       " -0.19117418,\n",
       " -0.18576121,\n",
       " -0.18177804,\n",
       " -0.18023619,\n",
       " -0.17626882,\n",
       " -0.1778399,\n",
       " -0.17715144,\n",
       " -0.1716052,\n",
       " -0.1645253,\n",
       " -0.16181499,\n",
       " -0.15264851,\n",
       " -0.13881624,\n",
       " -0.132063,\n",
       " -0.13914123,\n",
       " -0.15027764,\n",
       " -0.16565874,\n",
       " -0.18163201,\n",
       " -0.19600311,\n",
       " -0.20832425,\n",
       " -0.21301225,\n",
       " -0.21272367,\n",
       " -0.21067365,\n",
       " -0.21026774,\n",
       " -0.20075746,\n",
       " -0.18399012,\n",
       " -0.16526985,\n",
       " -0.14549801,\n",
       " -0.12749097,\n",
       " -0.11589542,\n",
       " -0.11076081,\n",
       " -0.11880064,\n",
       " -0.13471717,\n",
       " -0.15391436,\n",
       " -0.16479778,\n",
       " -0.17665961,\n",
       " 0.0363414,\n",
       " 0.04393807,\n",
       " 0.043419152,\n",
       " 0.04776022,\n",
       " 0.051481456,\n",
       " 0.05876717,\n",
       " 0.06245449,\n",
       " 0.067861885,\n",
       " 0.07613227,\n",
       " 0.08564672,\n",
       " 0.09228572,\n",
       " 0.09423843,\n",
       " 0.09459856,\n",
       " 0.09847781,\n",
       " 0.097901195,\n",
       " 0.09480104,\n",
       " 0.09359899,\n",
       " 0.09181091,\n",
       " 0.09143129,\n",
       " 0.084970385,\n",
       " 0.08196893,\n",
       " 0.082145065,\n",
       " 0.07806221,\n",
       " 0.0731664,\n",
       " 0.07275656,\n",
       " 0.07683542,\n",
       " 0.0920445,\n",
       " 0.11092189,\n",
       " 0.12996915,\n",
       " 0.1420624,\n",
       " 0.14869198,\n",
       " 0.15384552,\n",
       " 0.15423861,\n",
       " 0.16109899,\n",
       " 0.16562751,\n",
       " 0.16529277,\n",
       " 0.16485056,\n",
       " 0.15613988,\n",
       " 0.13979557,\n",
       " 0.11568138,\n",
       " 0.100248784,\n",
       " 0.08927235,\n",
       " 0.08145258,\n",
       " 0.077893406,\n",
       " 0.04940912,\n",
       " 0.002056092,\n",
       " -0.060671,\n",
       " -0.10933784,\n",
       " -0.15084526,\n",
       " -0.18253791,\n",
       " -0.19888613,\n",
       " -0.21099643,\n",
       " -0.22592331,\n",
       " -0.23993531,\n",
       " -0.2525778,\n",
       " -0.25487903,\n",
       " -0.26036292,\n",
       " -0.26876828,\n",
       " -0.2754787,\n",
       " -0.27633804,\n",
       " -0.27457273,\n",
       " -0.28684863,\n",
       " -0.29606706,\n",
       " -0.30670297,\n",
       " -0.31012636,\n",
       " -0.31845433,\n",
       " -0.31918532,\n",
       " -0.3215276,\n",
       " -0.32116014,\n",
       " -0.3218655,\n",
       " -0.3256901,\n",
       " -0.32196048,\n",
       " -0.32927507,\n",
       " -0.33157343,\n",
       " -0.33632085,\n",
       " -0.33060116,\n",
       " -0.3203458,\n",
       " -0.3158784,\n",
       " -0.30971792,\n",
       " -0.3039115,\n",
       " -0.29285574,\n",
       " -0.28599232,\n",
       " -0.2798003,\n",
       " -0.2753988,\n",
       " -0.2706135,\n",
       " -0.27312136,\n",
       " -0.27823853,\n",
       " -0.27852064,\n",
       " -0.27264798,\n",
       " -0.26260608,\n",
       " -0.2588176,\n",
       " -0.2543596,\n",
       " -0.24446183,\n",
       " -0.2372269,\n",
       " -0.23796687,\n",
       " -0.23924349,\n",
       " -0.23765752,\n",
       " -0.23262084,\n",
       " -0.2362823,\n",
       " -0.23796074,\n",
       " -0.2405953,\n",
       " -0.24086253,\n",
       " -0.24055573,\n",
       " -0.24079303,\n",
       " -0.2373116,\n",
       " -0.2349862,\n",
       " -0.2325626,\n",
       " -0.23027359,\n",
       " -0.22742844,\n",
       " -0.22220114,\n",
       " -0.22437266,\n",
       " -0.21927692,\n",
       " -0.2170215,\n",
       " -0.21405497,\n",
       " -0.21278644,\n",
       " -0.217043,\n",
       " -0.21278495,\n",
       " -0.20909533,\n",
       " -0.20393264,\n",
       " -0.20287947,\n",
       " -0.20873484,\n",
       " -0.20626011,\n",
       " -0.20520303,\n",
       " -0.2039282,\n",
       " -0.2065138,\n",
       " -0.20294869,\n",
       " -0.192729,\n",
       " -0.18730673,\n",
       " -0.18099067,\n",
       " -0.1840348,\n",
       " -0.18149048,\n",
       " -0.17792988,\n",
       " -0.17437139,\n",
       " -0.16867098,\n",
       " -0.16804746,\n",
       " -0.16108456,\n",
       " -0.16182345,\n",
       " -0.16241455,\n",
       " -0.16585511,\n",
       " -0.16798827,\n",
       " -0.16442826,\n",
       " -0.16424567,\n",
       " -0.15917683,\n",
       " -0.1603503,\n",
       " -0.15361893,\n",
       " -0.15195122,\n",
       " -0.14619511,\n",
       " -0.14423898,\n",
       " -0.13993308,\n",
       " -0.1337598,\n",
       " -0.13627404,\n",
       " -0.13042805,\n",
       " -0.131215,\n",
       " -0.12634543,\n",
       " -0.121281475,\n",
       " -0.12057194,\n",
       " -0.117895246,\n",
       " -0.1208342,\n",
       " -0.12341395,\n",
       " -0.1249862,\n",
       " -0.12663966,\n",
       " -0.12547526,\n",
       " -0.11545336,\n",
       " -0.11489922,\n",
       " -0.109816074,\n",
       " -0.111812145,\n",
       " -0.11654389,\n",
       " -0.11509979,\n",
       " -0.11797398,\n",
       " -0.112240136,\n",
       " -0.10801706,\n",
       " -0.10478628,\n",
       " -0.09797266,\n",
       " -0.09763402,\n",
       " -0.09544131,\n",
       " -0.096835256,\n",
       " -0.1000053,\n",
       " -0.09709403,\n",
       " -0.08972168,\n",
       " -0.091608435,\n",
       " -0.09027177,\n",
       " -0.09462282,\n",
       " -0.0874376,\n",
       " -0.084212214,\n",
       " -0.07705638,\n",
       " -0.07388151,\n",
       " -0.07246885,\n",
       " -0.07300234,\n",
       " -0.070086986,\n",
       " -0.063394636,\n",
       " -0.05542633,\n",
       " -0.051324278,\n",
       " -0.051951647,\n",
       " -0.048575908,\n",
       " -0.042631954,\n",
       " -0.033397585,\n",
       " -0.02888015,\n",
       " -0.027376622,\n",
       " -0.024490505,\n",
       " -0.02812314,\n",
       " -0.026167095,\n",
       " -0.02758947,\n",
       " -0.02505061,\n",
       " -0.020765662,\n",
       " -0.018571675,\n",
       " -0.014913231,\n",
       " -0.010811865,\n",
       " -0.006819874,\n",
       " -0.0072995424,\n",
       " -0.008974165,\n",
       " -0.0065098405,\n",
       " 0.0058841407,\n",
       " 0.009674132,\n",
       " 0.013247073,\n",
       " 0.0135214925,\n",
       " 0.019213408,\n",
       " 0.02512768,\n",
       " 0.024904668,\n",
       " 0.02792564,\n",
       " 0.03011921,\n",
       " 0.035899073,\n",
       " 0.04021862,\n",
       " 0.041816384,\n",
       " 0.047647387,\n",
       " 0.0475505,\n",
       " 0.041809857,\n",
       " 0.039412916,\n",
       " 0.038360238,\n",
       " 0.038267016,\n",
       " 0.033492476,\n",
       " 0.03156683,\n",
       " 0.033383816,\n",
       " 0.03900981,\n",
       " 0.039384216,\n",
       " 0.034895808,\n",
       " 0.030445188,\n",
       " 0.029123694,\n",
       " 0.031017989,\n",
       " 0.030231029,\n",
       " 0.030263096,\n",
       " 0.02985382,\n",
       " 0.03278026,\n",
       " 0.032792747,\n",
       " 0.034575492,\n",
       " 0.03869173,\n",
       " 0.03920558,\n",
       " 0.03939259,\n",
       " 0.03829071,\n",
       " 0.04274738,\n",
       " 0.043589234,\n",
       " 0.045053035,\n",
       " 0.045074582,\n",
       " 0.049877495,\n",
       " 0.055799693,\n",
       " 0.059281677,\n",
       " 0.06256333,\n",
       " 0.06136653,\n",
       " 0.06581882,\n",
       " 0.066844374,\n",
       " 0.069126874,\n",
       " 0.07304016,\n",
       " 0.07677564,\n",
       " 0.08054915,\n",
       " 0.079876035,\n",
       " 0.08646163,\n",
       " 0.088052124,\n",
       " 0.09426895,\n",
       " 0.09771362,\n",
       " 0.101647586,\n",
       " 0.10986468,\n",
       " 0.116426915,\n",
       " 0.1245704,\n",
       " 0.12779221,\n",
       " 0.13567123,\n",
       " 0.13737848,\n",
       " 0.13699487,\n",
       " 0.13618842,\n",
       " 0.13391176,\n",
       " 0.13745013,\n",
       " 0.13719907,\n",
       " 0.13928255,\n",
       " 0.13884255,\n",
       " 0.14163896,\n",
       " 0.14627096,\n",
       " 0.14719883,\n",
       " 0.15384266,\n",
       " 0.15797594,\n",
       " 0.16775188,\n",
       " 0.17380956,\n",
       " 0.18163589,\n",
       " 0.18755719,\n",
       " 0.1908727,\n",
       " 0.18974313,\n",
       " 0.18537018,\n",
       " 0.18516383,\n",
       " 0.18776032,\n",
       " 0.19284996,\n",
       " 0.19607505,\n",
       " 0.19891933,\n",
       " 0.20645627,\n",
       " 0.22324869,\n",
       " 0.23777482,\n",
       " 0.2482203,\n",
       " 0.25862107,\n",
       " 0.2695262,\n",
       " 0.28055277,\n",
       " 0.28971812,\n",
       " 0.3000683,\n",
       " 0.31580886,\n",
       " 0.33078483,\n",
       " 0.33897784,\n",
       " 0.34527168,\n",
       " 0.34840378,\n",
       " 0.3589054,\n",
       " 0.37568566,\n",
       " 0.39301828,\n",
       " 0.42046615,\n",
       " 0.43562582,\n",
       " 0.436007,\n",
       " 0.40286633,\n",
       " 0.37681904,\n",
       " 0.34956685,\n",
       " 0.32678142,\n",
       " 0.29585418,\n",
       " 0.27628985,\n",
       " 0.26557943,\n",
       " 0.25365922,\n",
       " 0.2412971,\n",
       " 0.22479758,\n",
       " 0.21209845,\n",
       " 0.1995748,\n",
       " 0.18286529,\n",
       " 0.17053846,\n",
       " 0.15803787,\n",
       " 0.15187636,\n",
       " 0.13755617,\n",
       " 0.13025203,\n",
       " 0.12651238,\n",
       " 0.12577501,\n",
       " 0.12404129,\n",
       " 0.11670479,\n",
       " 0.11040881,\n",
       " 0.10837671,\n",
       " 0.10940918,\n",
       " 0.11885026,\n",
       " 0.1319575,\n",
       " 0.14437535,\n",
       " 0.16021183,\n",
       " 0.17213628,\n",
       " 0.18228409,\n",
       " 0.18953362,\n",
       " 0.19750115,\n",
       " 0.20568195,\n",
       " 0.21928069,\n",
       " 0.22960153,\n",
       " 0.23878929,\n",
       " 0.25329813,\n",
       " 0.26022092,\n",
       " 0.2728342,\n",
       " 0.2717643,\n",
       " 0.26646528,\n",
       " 0.2603568,\n",
       " 0.25279132,\n",
       " 0.24130431,\n",
       " 0.22168323,\n",
       " 0.2066122,\n",
       " 0.19144687,\n",
       " 0.18608335,\n",
       " 0.17272916,\n",
       " 0.15195677,\n",
       " 0.12904921,\n",
       " 0.11448631,\n",
       " 0.09932074,\n",
       " 0.085188,\n",
       " 0.07545945,\n",
       " 0.07264802,\n",
       " 0.06670651,\n",
       " 0.0567455,\n",
       " 0.040235996,\n",
       " 0.018722713,\n",
       " -0.007283002,\n",
       " -0.027498603,\n",
       " -0.049263567,\n",
       " -0.0663864,\n",
       " -0.08979949,\n",
       " -0.10294631,\n",
       " -0.11777443,\n",
       " -0.117396444,\n",
       " -0.1222381,\n",
       " -0.12752333,\n",
       " -0.13633421,\n",
       " -0.14074469,\n",
       " -0.13832727,\n",
       " -0.14507481,\n",
       " -0.15564886,\n",
       " -0.16325274,\n",
       " -0.16384465,\n",
       " -0.16660514,\n",
       " -0.17051941,\n",
       " -0.1766803,\n",
       " -0.18140107,\n",
       " -0.18581724,\n",
       " -0.19494358,\n",
       " -0.20477378,\n",
       " -0.20891148,\n",
       " -0.2050119,\n",
       " -0.19985339,\n",
       " -0.1958957,\n",
       " -0.19051868,\n",
       " -0.18501785,\n",
       " -0.17859262,\n",
       " -0.1776185,\n",
       " -0.18042403,\n",
       " -0.17168316,\n",
       " -0.16445735,\n",
       " -0.16675633,\n",
       " -0.16642639,\n",
       " -0.15929767,\n",
       " -0.15477297,\n",
       " -0.15620998,\n",
       " -0.15473047,\n",
       " -0.14579156,\n",
       " -0.14576331,\n",
       " -0.15203753,\n",
       " -0.16399434,\n",
       " -0.18195298,\n",
       " -0.20133789,\n",
       " -0.21822467,\n",
       " -0.23062693,\n",
       " -0.23590073,\n",
       " -0.23576461,\n",
       " -0.23423365,\n",
       " -0.23374629,\n",
       " -0.22488716,\n",
       " -0.20908406,\n",
       " -0.1930446,\n",
       " -0.17653999,\n",
       " -0.16142994,\n",
       " -0.1489752,\n",
       " -0.14239177,\n",
       " -0.1484339,\n",
       " -0.16045204,\n",
       " -0.1784096,\n",
       " -0.1907365,\n",
       " -0.20212513,\n",
       " 0.038274437,\n",
       " 0.037869394,\n",
       " 0.0395647,\n",
       " 0.043071747,\n",
       " 0.048273683,\n",
       " 0.052000433,\n",
       " 0.057272762,\n",
       " 0.06605527,\n",
       " 0.075268656,\n",
       " 0.081167966,\n",
       " 0.081787795,\n",
       " 0.082190424,\n",
       " 0.08597192,\n",
       " 0.08430669,\n",
       " 0.080258876,\n",
       " 0.078323096,\n",
       " 0.07592866,\n",
       " 0.07562271,\n",
       " 0.071109146,\n",
       " 0.069692105,\n",
       " 0.065930516,\n",
       " 0.06317219,\n",
       " 0.05900219,\n",
       " 0.05882266,\n",
       " 0.06200275,\n",
       " 0.071510404,\n",
       " 0.089194864,\n",
       " 0.107714325,\n",
       " 0.11861351,\n",
       " 0.12373403,\n",
       " 0.12540206,\n",
       " 0.12509707,\n",
       " 0.1302652,\n",
       " 0.13549164,\n",
       " 0.13915989,\n",
       " 0.13984308,\n",
       " 0.13187918,\n",
       " 0.11659762,\n",
       " 0.09374812,\n",
       " 0.078591436,\n",
       " 0.06587067,\n",
       " 0.05684212,\n",
       " 0.046316206,\n",
       " 0.016812772,\n",
       " -0.031552225,\n",
       " -0.085045904,\n",
       " -0.13609877,\n",
       " -0.1786429,\n",
       " -0.20695578,\n",
       " -0.21986318,\n",
       " -0.23196223,\n",
       " -0.24781285,\n",
       " -0.26145178,\n",
       " -0.2739794,\n",
       " -0.27651834,\n",
       " -0.28303632,\n",
       " -0.28804234,\n",
       " -0.29595983,\n",
       " -0.29664463,\n",
       " -0.295308,\n",
       " -0.3057183,\n",
       " -0.3134483,\n",
       " -0.3246923,\n",
       " -0.3283369,\n",
       " -0.33758476,\n",
       " -0.3389803,\n",
       " -0.34114146,\n",
       " -0.34092903,\n",
       " -0.34175542,\n",
       " -0.34553885,\n",
       " -0.3414387,\n",
       " -0.34938043,\n",
       " -0.3510204,\n",
       " -0.35623696,\n",
       " -0.349696,\n",
       " -0.338173,\n",
       " -0.3322479,\n",
       " -0.3255217,\n",
       " -0.3204732,\n",
       " -0.31173465,\n",
       " -0.30511218,\n",
       " -0.29845226,\n",
       " -0.2935428,\n",
       " -0.288804,\n",
       " -0.2917209,\n",
       " -0.29771343,\n",
       " -0.29854748,\n",
       " -0.2932149,\n",
       " -0.28399605,\n",
       " -0.28089985,\n",
       " -0.2771986,\n",
       " -0.26753938,\n",
       " -0.26107073,\n",
       " -0.26116177,\n",
       " -0.26220414,\n",
       " -0.2608591,\n",
       " -0.2559107,\n",
       " -0.25987017,\n",
       " -0.26143044,\n",
       " -0.26426324,\n",
       " -0.26466474,\n",
       " -0.2640801,\n",
       " -0.26446775,\n",
       " -0.2602755,\n",
       " -0.25773177,\n",
       " -0.2554823,\n",
       " -0.25314534,\n",
       " -0.25071704,\n",
       " -0.24523047,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
